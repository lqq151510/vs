{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab9d3604",
   "metadata": {},
   "source": [
    "# 逻辑回归\n",
    "\n",
    "逻辑回归是一种广泛应用于分类任务的线性模型，尽管名字中带有“回归”，但它主要用于解决二分类和多分类问题。\n",
    "\n",
    "## 原理\n",
    "逻辑回归通过学习特征的线性组合，使用 Sigmoid 函数将结果映射到概率空间，从而实现分类。\n",
    "\n",
    "### Sigmoid 函数\n",
    "\\[\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\]\n",
    "其中：\n",
    "- \\( z = w \\cdot x + b \\)\n",
    "- \\( w \\) 是权重向量，\\( b \\) 是偏置。\n",
    "\n",
    "### 分类规则\n",
    "- 当 \\( \\sigma(z) \\geq 0.5 \\) 时，预测为正类。\n",
    "- 当 \\( \\sigma(z) < 0.5 \\) 时，预测为负类。\n",
    "\n",
    "## 优点\n",
    "1. **简单高效**：计算复杂度低，易于实现。\n",
    "2. **概率输出**：能够输出样本属于某一类别的概率。\n",
    "3. **可解释性强**：权重可以解释特征对分类结果的影响。\n",
    "4. **适用性广**：适用于二分类和多分类任务。\n",
    "\n",
    "## 缺点\n",
    "1. **线性假设**：假设特征与目标变量之间是线性关系，可能不适用于复杂的非线性问题。\n",
    "2. **对异常值敏感**：异常值可能显著影响模型性能。\n",
    "3. **需要特征工程**：通常需要对特征进行标准化或归一化处理。\n",
    "\n",
    "## 应用场景\n",
    "- 医学诊断\n",
    "- 广告点击率预测\n",
    "- 信用评分\n",
    "- 文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08dce4",
   "metadata": {},
   "source": [
    "这段代码创建了一个逻辑回归模型实例，具体参数的含义如下：\n",
    "\n",
    "1. **`max_iter=1000`**：\n",
    "   - 指定优化算法的最大迭代次数。\n",
    "   - 在逻辑回归中，优化算法（如梯度下降）通过迭代来寻找最优解。如果迭代次数不足，可能会导致模型未能完全收敛。\n",
    "   - 设置为 1000 表示最多允许 1000 次迭代，以确保模型能够收敛。\n",
    "\n",
    "2. **`multi_class='multinomial'`**：\n",
    "   - 指定逻辑回归的多分类策略。\n",
    "   - `'multinomial'` 表示使用多项式逻辑回归（Softmax 回归），适用于多分类任务。\n",
    "   - 与之相对，`'ovr'`（one-vs-rest）策略会将多分类问题拆分为多个二分类问题。\n",
    "\n",
    "3. **`solver='lbfgs'`**：\n",
    "   - 指定优化算法的类型。\n",
    "   - `'lbfgs'` 是一种拟牛顿法优化算法，适合处理多分类问题和中小规模数据集。\n",
    "   - 它具有较快的收敛速度，能够高效处理逻辑回归的优化问题。\n",
    "\n",
    "这段代码的作用是创建一个逻辑回归模型，适用于多分类任务，并使用 `lbfgs` 优化算法进行训练，同时设置了较高的最大迭代次数以确保模型的收敛性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15462cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba803b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "逻辑回归模型在测试集上的准确率: 1.00\n",
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuyongze/vs/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 加载鸢尾花数据集\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建逻辑回归模型\n",
    "log_reg = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "# 训练模型\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"逻辑回归模型在测试集上的准确率: {accuracy:.2f}\")\n",
    "\n",
    "# 输出分类报告\n",
    "print(\"分类报告:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d9941",
   "metadata": {},
   "source": [
    "### 结果分析\n",
    "通过逻辑回归模型对鸢尾花数据集进行分类，模型的准确率和分类报告反映了其在测试集上的表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f63070c",
   "metadata": {},
   "source": [
    "# 逻辑回归的正则化\n",
    "\n",
    "逻辑回归支持两种正则化方法，用于防止过拟合：\n",
    "1. **L1 正则化**：\n",
    "   - 通过惩罚权重的绝对值，鼓励稀疏解。\n",
    "   - 适用于特征选择。\n",
    "\n",
    "2. **L2 正则化**：\n",
    "   - 通过惩罚权重的平方和，限制权重的大小。\n",
    "   - 适用于防止过拟合。\n",
    "\n",
    "### 损失函数\n",
    "逻辑回归的损失函数为对数损失函数，加上正则化项：\n",
    "\\[\n",
    "L(w) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] + \\lambda R(w)\n",
    "\\]\n",
    "其中：\n",
    "- \\( R(w) \\) 是正则化项，L1 正则化为 \\( \\|w\\|_1 \\)，L2 正则化为 \\( \\|w\\|_2^2 \\)。\n",
    "- \\( \\lambda \\) 是正则化强度，控制正则化项的权重。\n",
    "\n",
    "### 注意事项\n",
    "- 正则化强度 \\( \\lambda \\) 需要通过交叉验证进行调优。\n",
    "- 特征值需要进行标准化或归一化处理，以避免正则化对不同量纲的特征产生不公平的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff5bb6c",
   "metadata": {},
   "source": [
    "# 逻辑回归常用算法策略\n",
    "\n",
    "逻辑回归是一种广泛应用的分类算法，以下是其常用的算法策略：\n",
    "\n",
    "## 1. **梯度下降法 (Gradient Descent)**\n",
    "- **原理**：\n",
    "  - 通过迭代优化损失函数，逐步更新模型参数，使得损失函数值最小化。\n",
    "- **特点**：\n",
    "  - 适用于大规模数据集。\n",
    "  - 学习率的选择对收敛速度和结果有重要影响。\n",
    "- **变体**：\n",
    "  - 批量梯度下降 (Batch Gradient Descent)\n",
    "  - 随机梯度下降 (Stochastic Gradient Descent, SGD)\n",
    "  - 小批量梯度下降 (Mini-batch Gradient Descent)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **牛顿法 (Newton's Method)**\n",
    "- **原理**：\n",
    "  - 使用二阶导数（Hessian 矩阵）来加速优化过程。\n",
    "- **特点**：\n",
    "  - 收敛速度快，适用于小规模数据集。\n",
    "  - 计算 Hessian 矩阵的代价较高，不适合高维数据。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **拟牛顿法 (Quasi-Newton Method)**\n",
    "- **原理**：\n",
    "  - 使用近似的 Hessian 矩阵来替代牛顿法中的精确 Hessian 矩阵。\n",
    "- **特点**：\n",
    "  - 计算效率较高，适用于中小规模数据集。\n",
    "  - 常用算法：BFGS、L-BFGS。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **坐标下降法 (Coordinate Descent)**\n",
    "- **原理**：\n",
    "  - 每次优化一个参数，固定其他参数不变，逐步优化所有参数。\n",
    "- **特点**：\n",
    "  - 适用于稀疏数据和 L1 正则化问题。\n",
    "  - 收敛速度较慢。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **随机优化法**\n",
    "- **原理**：\n",
    "  - 使用随机方法（如随机梯度下降）来优化模型参数。\n",
    "- **特点**：\n",
    "  - 能够跳出局部最优解。\n",
    "  - 适用于大规模数据集。\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **正则化策略**\n",
    "- **L1 正则化**：\n",
    "  - 通过惩罚权重的绝对值，鼓励稀疏解。\n",
    "  - 适用于特征选择。\n",
    "- **L2 正则化**：\n",
    "  - 通过惩罚权重的平方和，限制权重的大小。\n",
    "  - 适用于防止过拟合。\n",
    "\n",
    "---\n",
    "\n",
    "### 注意事项\n",
    "- 算法的选择取决于数据规模、特征维度和计算资源。\n",
    "- 对于大规模数据集，随机梯度下降和小批量梯度下降是常用的选择。\n",
    "- 对于稀疏数据，L1 正则化和坐标下降法表现较好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5efc37e",
   "metadata": {},
   "source": [
    "# 逻辑回归中的 solver 参数\n",
    "\n",
    "在逻辑回归中，`solver` 参数用于指定优化算法，不同的 `solver` 适用于不同的数据规模和问题类型。以下是常用 `solver` 的说明：\n",
    "\n",
    "## 1. **`lbfgs`**\n",
    "- **全称**：Limited-memory Broyden–Fletcher–Goldfarb–Shanno\n",
    "- **特点**：\n",
    "  - 一种拟牛顿法优化算法。\n",
    "  - 适用于中小规模数据集。\n",
    "  - 支持多分类问题（`multi_class='multinomial'`）。\n",
    "- **优点**：\n",
    "  - 收敛速度快，计算效率高。\n",
    "- **缺点**：\n",
    "  - 对大规模数据集可能不适用。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **`newton-cg`**\n",
    "- **全称**：Newton Conjugate Gradient\n",
    "- **特点**：\n",
    "  - 使用牛顿法结合共轭梯度优化。\n",
    "  - 适用于中小规模数据集。\n",
    "  - 支持多分类问题（`multi_class='multinomial'`）。\n",
    "- **优点**：\n",
    "  - 对于高维数据表现良好。\n",
    "- **缺点**：\n",
    "  - 计算 Hessian 矩阵的代价较高。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **`sag`**\n",
    "- **全称**：Stochastic Average Gradient\n",
    "- **特点**：\n",
    "  - 一种随机优化算法。\n",
    "  - 适用于大规模数据集。\n",
    "  - 仅支持 L2 正则化。\n",
    "- **优点**：\n",
    "  - 对大规模数据集的收敛速度较快。\n",
    "- **缺点**：\n",
    "  - 对稀疏数据的支持较差。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **`saga`**\n",
    "- **全称**：Stochastic Average Gradient Augmented\n",
    "- **特点**：\n",
    "  - `sag` 的改进版本。\n",
    "  - 适用于大规模数据集。\n",
    "  - 支持 L1 和 L2 正则化。\n",
    "- **优点**：\n",
    "  - 能够处理稀疏数据。\n",
    "- **缺点**：\n",
    "  - 对小规模数据集可能表现不佳。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **`liblinear`**\n",
    "- **特点**：\n",
    "  - 基于坐标下降法的优化算法。\n",
    "  - 适用于小规模数据集。\n",
    "  - 支持 L1 和 L2 正则化。\n",
    "- **优点**：\n",
    "  - 对稀疏数据表现良好。\n",
    "- **缺点**：\n",
    "  - 不支持多分类问题（`multi_class='multinomial'`）。\n",
    "\n",
    "---\n",
    "\n",
    "### 总结\n",
    "| Solver      | 适用场景                  | 支持正则化 | 支持多分类 | 优点                     |\n",
    "|-------------|---------------------------|------------|------------|--------------------------|\n",
    "| `lbfgs`     | 中小规模数据集            | L2         | 是         | 收敛速度快，效率高       |\n",
    "| `newton-cg` | 中小规模数据集            | L2         | 是         | 高维数据表现良好         |\n",
    "| `sag`       | 大规模数据集              | L2         | 是         | 大规模数据收敛速度快     |\n",
    "| `saga`      | 大规模数据集，稀疏数据    | L1, L2     | 是         | 支持稀疏数据             |\n",
    "| `liblinear` | 小规模数据集，稀疏数据    | L1, L2     | 否         | 对稀疏数据表现良好       |\n",
    "\n",
    "### 注意事项\n",
    "- 根据数据规模和特征选择合适的 `solver`。\n",
    "- 对于多分类问题，推荐使用 `lbfgs`、`newton-cg` 或 `saga`。\n",
    "- 对于大规模数据集，推荐使用 `sag` 或 `saga`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5e54c1",
   "metadata": {},
   "source": [
    "# 逻辑回归的损失函数\n",
    "\n",
    "逻辑回归的目标是通过最小化损失函数来优化模型参数。以下是逻辑回归的损失函数及其相关内容：\n",
    "\n",
    "## 1. 对数损失函数 (Log Loss)\n",
    "逻辑回归使用对数损失函数来衡量模型的预测误差。对于二分类问题，其损失函数定义为：\n",
    "\\[\n",
    "L(w, b) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "\\]\n",
    "其中：\n",
    "- \\( n \\)：样本数量。\n",
    "- \\( y_i \\)：第 \\(i\\) 个样本的真实标签，取值为 0 或 1。\n",
    "- \\( \\hat{y}_i \\)：第 \\(i\\) 个样本的预测概率，计算公式为：\n",
    "  \\[\n",
    "  \\hat{y}_i = \\sigma(w \\cdot x_i + b) = \\frac{1}{1 + e^{-(w \\cdot x_i + b)}}\n",
    "  \\]\n",
    "- \\( w \\)：权重向量。\n",
    "- \\( b \\)：偏置。\n",
    "\n",
    "### 损失解释\n",
    "1. 当 \\( y_i = 1 \\) 时，损失为 \\( -\\log(\\hat{y}_i) \\)，即预测概率越接近 1，损失越小。\n",
    "2. 当 \\( y_i = 0 \\) 时，损失为 \\( -\\log(1 - \\hat{y}_i) \\)，即预测概率越接近 0，损失越小。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 正则化项\n",
    "为了防止过拟合，逻辑回归通常会在损失函数中加入正则化项。常见的正则化方法包括：\n",
    "1. **L1 正则化**：\n",
    "   - 惩罚权重的绝对值，鼓励稀疏解。\n",
    "   - 正则化项为 \\( \\lambda \\|w\\|_1 \\)。\n",
    "\n",
    "2. **L2 正则化**：\n",
    "   - 惩罚权重的平方和，限制权重的大小。\n",
    "   - 正则化项为 \\( \\lambda \\|w\\|_2^2 \\)。\n",
    "\n",
    "加入正则化后的损失函数为：\n",
    "\\[\n",
    "L(w, b) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] + \\lambda R(w)\n",
    "\\]\n",
    "其中：\n",
    "- \\( R(w) \\) 是正则化项。\n",
    "- \\( \\lambda \\) 是正则化强度，控制正则化项的权重。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 多分类问题\n",
    "对于多分类问题，逻辑回归使用交叉熵损失函数：\n",
    "\\[\n",
    "L(w, b) = -\\frac{1}{n} \\sum_{i=1}^n \\sum_{k=1}^K y_{i,k} \\log(\\hat{y}_{i,k})\n",
    "\\]\n",
    "其中：\n",
    "- \\( K \\)：类别数量。\n",
    "- \\( y_{i,k} \\)：第 \\(i\\) 个样本属于第 \\(k\\) 类的真实标签（取值为 0 或 1）。\n",
    "- \\( \\hat{y}_{i,k} \\)：第 \\(i\\) 个样本属于第 \\(k\\) 类的预测概率。\n",
    "\n",
    "---\n",
    "\n",
    "### 注意事项\n",
    "1. **正则化强度 \\( \\lambda \\)**：\n",
    "   - 需要通过交叉验证进行调优。\n",
    "2. **特征预处理**：\n",
    "   - 特征值需要进行标准化或归一化处理，以避免正则化对不同量纲的特征产生不公平的影响。\n",
    "3. **数值稳定性**：\n",
    "   - 在计算 \\( \\log(\\hat{y}_i) \\) 时，需避免 \\( \\hat{y}_i \\) 过于接近 0 或 1，通常通过加一个小的常数 \\( \\epsilon \\) 来处理。\n",
    "\n",
    "---\n",
    "\n",
    "### 总结\n",
    "逻辑回归的损失函数通过最小化对数损失来优化模型参数，同时通过正则化项控制模型复杂度，从而提高模型的泛化能力。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
