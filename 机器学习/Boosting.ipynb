{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52cc09ad",
   "metadata": {},
   "source": [
    "# Boosting 算法\n",
    "Boosting 是一种集成学习方法，通过将多个弱学习器组合成一个强学习器来提高模型的准确性。\n",
    "\n",
    "## 原理\n",
    "Boosting 的核心思想是通过迭代地训练弱学习器，每次迭代时关注前一轮中被错误分类的样本，从而逐步提高模型的性能。\n",
    "\n",
    "### 主要步骤：\n",
    "1. **初始化样本权重**：\n",
    "   - 为每个样本分配一个初始权重，通常为相等的权重。\n",
    "\n",
    "2. **训练弱学习器**：\n",
    "   - 使用当前样本权重训练一个弱学习器。\n",
    "   - 计算弱学习器的错误率。\n",
    "\n",
    "3. **更新样本权重**：\n",
    "   - 增加被错误分类样本的权重，减少被正确分类样本的权重。\n",
    "   - 使得下一轮训练更加关注被错误分类的样本。\n",
    "\n",
    "4. **组合弱学习器**：\n",
    "   - 根据每个弱学习器的性能（如错误率）分配权重。\n",
    "   - 将所有弱学习器的预测结果加权组合，得到最终的强学习器。\n",
    "\n",
    "## 常见算法\n",
    "1. **AdaBoost**：\n",
    "   - 通过调整样本权重来训练多个弱学习器。\n",
    "   - 使用加权投票或加权平均的方法组合弱学习器。\n",
    "\n",
    "2. **Gradient Boosting**：\n",
    "   - 通过优化损失函数的梯度来训练多个弱学习器。\n",
    "   - 每次迭代时，拟合当前模型的残差。\n",
    "\n",
    "3. **XGBoost**：\n",
    "   - 基于 Gradient Boosting 的改进版本，具有更高的效率和更强的性能。\n",
    "   - 支持正则化，能够有效防止过拟合。\n",
    "\n",
    "## 优点\n",
    "- **高准确性**：Boosting 能够显著提高模型的准确性。\n",
    "- **灵活性**：可以与多种弱学习器结合使用。\n",
    "- **处理不平衡数据**：通过调整样本权重，Boosting 能够更好地处理不平衡数据。\n",
    "\n",
    "## 缺点\n",
    "- **计算成本高**：Boosting 是一个迭代过程，训练时间较长。\n",
    "- **对噪声敏感**：Boosting 可能会过度拟合噪声数据。\n",
    "\n",
    "## 应用场景\n",
    "- Boosting 广泛应用于分类和回归任务，如文本分类、图像识别和金融预测等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad2354",
   "metadata": {},
   "source": [
    "# Boosting 和 Bagging 的区别\n",
    "Boosting 和 Bagging 是两种常见的集成学习方法，它们通过结合多个弱学习器来提高模型的性能，但在原理和实现上存在显著区别。\n",
    "\n",
    "## 1. 核心思想\n",
    "- **Boosting**：\n",
    "  - 通过序列化地训练多个弱学习器，降低模型的偏差。\n",
    "  - 每个弱学习器依赖于前一个弱学习器的结果，逐步优化模型性能。\n",
    "\n",
    "- **Bagging**：\n",
    "  - 通过并行训练多个弱学习器，降低模型的方差。\n",
    "  - 每个弱学习器独立训练，最终通过投票（分类任务）或平均（回归任务）进行结果集成。\n",
    "\n",
    "## 2. 数据采样\n",
    "- **Boosting**：\n",
    "  - 不进行随机采样，而是根据前一轮的错误分类样本调整样本权重。\n",
    "  - 被错误分类的样本权重会增加，使得下一轮训练更加关注这些样本。\n",
    "\n",
    "- **Bagging**：\n",
    "  - 使用有放回的随机采样生成多个子数据集，每个子数据集的大小与原始数据集相同。\n",
    "  - 子数据集之间相互独立。\n",
    "\n",
    "## 3. 弱学习器的训练\n",
    "- **Boosting**：\n",
    "  - 弱学习器的训练是串行的，每个弱学习器依赖于前一个弱学习器的结果。\n",
    "\n",
    "- **Bagging**：\n",
    "  - 弱学习器的训练是并行的，彼此之间没有依赖关系。\n",
    "\n",
    "## 4. 结果集成\n",
    "- **Boosting**：\n",
    "  - 根据每个弱学习器的性能分配权重，使用加权投票或加权平均的方法进行结果集成。\n",
    "\n",
    "- **Bagging**：\n",
    "  - 对于分类任务，使用多数投票法集成多个弱学习器的预测结果。\n",
    "  - 对于回归任务，使用平均法集成多个弱学习器的预测结果。\n",
    "\n",
    "## 5. 适用场景\n",
    "- **Boosting**：\n",
    "  - 适用于高偏差的模型，能够有效降低偏差，提升模型的准确性。\n",
    "  - 典型应用：AdaBoost、Gradient Boosting、XGBoost。\n",
    "\n",
    "- **Bagging**：\n",
    "  - 适用于高方差的模型（如决策树），能够有效降低方差，提升模型的稳定性。\n",
    "  - 典型应用：随机森林。\n",
    "\n",
    "## 6. 计算成本\n",
    "- **Boosting**：\n",
    "  - 由于弱学习器的训练是串行的，计算效率较低。\n",
    "\n",
    "- **Bagging**：\n",
    "  - 并行训练多个弱学习器，计算效率较高。\n",
    "\n",
    "### 总结\n",
    "| 特性            | Boosting                    | Bagging                     |\n",
    "|-----------------|-----------------------------|-----------------------------|\n",
    "| 核心目标        | 降低偏差                   | 降低方差                   |\n",
    "| 数据采样        | 根据错误分类调整样本权重   | 有放回随机采样             |\n",
    "| 弱学习器训练    | 串行                       | 并行                       |\n",
    "| 结果集成        | 加权投票或加权平均         | 投票或平均                 |\n",
    "| 计算成本        | 较高                       | 较低                       |\n",
    "| 典型应用        | AdaBoost、Gradient Boosting | 随机森林                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23d4b8",
   "metadata": {},
   "source": [
    "# AdaBoost 运算案例\n",
    "使用 AdaBoost 算法对鸢尾花数据集进行分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d210237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32c70025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost 模型在测试集上的准确率: 0.93\n"
     ]
    }
   ],
   "source": [
    "# 加载鸢尾花数据集\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建 AdaBoost 模型\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)  # 使用深度为 1 的决策树作为弱学习器\n",
    "adaboost_clf = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = adaboost_clf.predict(X_test)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"AdaBoost 模型在测试集上的准确率: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728cbe2",
   "metadata": {},
   "source": [
    "### 结果分析\n",
    "通过 AdaBoost 模型对鸢尾花数据集进行分类，模型的准确率反映了其在测试集上的表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddbeda3",
   "metadata": {},
   "source": [
    "# GBDT 算法\n",
    "GBDT（Gradient Boosting Decision Tree，梯度提升决策树）是一种基于梯度提升框架的集成学习算法，广泛应用于分类和回归任务。\n",
    "\n",
    "## 原理\n",
    "GBDT 的核心思想是通过迭代地训练决策树，每次迭代时拟合当前模型的残差（即预测值与真实值之间的差异），从而逐步优化模型性能。\n",
    "\n",
    "### 主要步骤：\n",
    "1. **初始化模型**：\n",
    "   - 使用一个简单的模型（如常数值）初始化预测值。\n",
    "\n",
    "2. **计算残差**：\n",
    "   - 在每次迭代中，计算当前模型的残差，作为下一棵决策树的目标值。\n",
    "\n",
    "3. **训练决策树**：\n",
    "   - 使用残差作为目标值，训练一棵新的决策树。\n",
    "\n",
    "4. **更新模型**：\n",
    "   - 将新决策树的预测结果加权后加入当前模型，更新模型的预测值。\n",
    "\n",
    "5. **重复迭代**：\n",
    "   - 重复上述步骤，直到达到预定的迭代次数或模型的误差满足要求。\n",
    "\n",
    "## 优点\n",
    "- **高准确性**：GBDT 能够有效拟合复杂的非线性关系。\n",
    "- **灵活性**：支持多种损失函数（如均方误差、对数损失等）。\n",
    "- **特征选择**：GBDT 能够评估特征的重要性，便于特征选择。\n",
    "\n",
    "## 缺点\n",
    "- **计算成本高**：GBDT 是一个迭代过程，训练时间较长。\n",
    "- **对参数敏感**：需要仔细调整超参数（如学习率、树的深度等）。\n",
    "- **对噪声敏感**：可能会过拟合噪声数据。\n",
    "\n",
    "## 应用场景\n",
    "- GBDT 广泛应用于分类和回归任务，如信用评分、点击率预测和风险评估等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7e60ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a048f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT 模型在测试集上的准确率: 1.00\n"
     ]
    }
   ],
   "source": [
    "# 加载鸢尾花数据集\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建 GBDT 模型\n",
    "gbdt_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "gbdt_clf.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = gbdt_clf.predict(X_test)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"GBDT 模型在测试集上的准确率: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba23b54",
   "metadata": {},
   "source": [
    "### 结果分析\n",
    "通过 GBDT 模型对鸢尾花数据集进行分类，模型的准确率反映了其在测试集上的表现。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
