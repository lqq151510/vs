{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c35630a",
   "metadata": {},
   "source": [
    "# 决策树学习\n",
    "决策树是一种常用的监督学习算法，适用于分类和回归任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ac76d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型准确率: 1.00\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载数据集\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建并训练决策树模型\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 测试模型\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f'模型准确率: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2481e7c0",
   "metadata": {},
   "source": [
    "## 信息熵、信息增益率和基尼系数\n",
    "在决策树中，信息熵用于衡量数据的不确定性，信息增益率用于选择最优划分属性，基尼系数用于评估数据集的纯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86937c4",
   "metadata": {},
   "source": [
    "## 信息熵、信息增益率和基尼系数的定义和公式\n",
    "### 信息熵 (Entropy)\n",
    "信息熵是衡量数据集不确定性的一种指标，其公式为：\n",
    "$$ Entropy = -\\sum_{i=1}^{n} p_i \\log_2(p_i) $$\n",
    "其中，$p_i$ 是类别 $i$ 的概率。\n",
    "\n",
    "### 信息增益率 (Information Gain Ratio)\n",
    "信息增益率用于衡量某一特征对数据集划分的有效性，其公式为：\n",
    "$$ GainRatio = \\frac{Gain}{IV} $$\n",
    "其中，$Gain$ 是信息增益，$IV$ 是固有值 (Intrinsic Value)。\n",
    "\n",
    "### 基尼系数 (Gini Index)\n",
    "基尼系数用于衡量数据集的纯度，其公式为：\n",
    "$$ Gini = 1 - \\sum_{i=1}^{n} p_i^2 $$\n",
    "其中，$p_i$ 是类别 $i$ 的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed71f0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信息熵: 0.9852\n"
     ]
    }
   ],
   "source": [
    "# 计算信息熵\n",
    "#当数据量一定时，系统越有序，熵值越低，系统越混乱，熵值越高\n",
    "from math import log2\n",
    "\n",
    "def entropy(labels):\n",
    "    total = len(labels)\n",
    "    counts = {label: labels.count(label) for label in set(labels)}\n",
    "    return -sum((count / total) * log2(count / total) for count in counts.values())\n",
    "\n",
    "# 示例数据\n",
    "labels = [0, 0, 1, 1, 1, 0, 1]\n",
    "print(f'信息熵: {entropy(labels):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a80f5f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基尼系数: 0.4898\n"
     ]
    }
   ],
   "source": [
    "# 计算基尼系数\n",
    "def gini(labels):\n",
    "    total = len(labels)\n",
    "    counts = {label: labels.count(label) for label in set(labels)}\n",
    "    return 1 - sum((count / total) ** 2 for count in counts.values())\n",
    "\n",
    "# 示例数据\n",
    "print(f'基尼系数: {gini(labels):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce62d547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信息增益率: 0.1300\n"
     ]
    }
   ],
   "source": [
    "# 计算信息增益率\n",
    "def information_gain_ratio(parent_entropy, subsets):\n",
    "    total = sum(len(subset) for subset in subsets)\n",
    "    weighted_entropy = sum((len(subset) / total) * entropy(subset) for subset in subsets)\n",
    "    intrinsic_value = -sum((len(subset) / total) * log2(len(subset) / total) for subset in subsets if len(subset) > 0)\n",
    "    gain = parent_entropy - weighted_entropy\n",
    "    return gain / intrinsic_value if intrinsic_value != 0 else 0\n",
    "\n",
    "# 示例数据\n",
    "subset1 = [0, 0, 1]\n",
    "subset2 = [1, 1, 0, 1]\n",
    "parent_entropy = entropy(labels)\n",
    "gain_ratio = information_gain_ratio(parent_entropy, [subset1, subset2])\n",
    "print(f'信息增益率: {gain_ratio:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07ce25e",
   "metadata": {},
   "source": [
    "## 信息增益率的定义和公式\n",
    "信息增益率是衡量某一特征对数据集划分效果的一种指标，其公式为：\n",
    "$$ GainRatio = \\frac{Gain}{IV} $$\n",
    "其中：\n",
    "- $Gain$ 是信息增益。\n",
    "- $IV$ 是固有值 (Intrinsic Value)，其公式为：\n",
    "$$ IV = -\\sum_{i=1}^{k} \\frac{|D_i|}{|D|} \\log_2 \\frac{|D_i|}{|D|} $$\n",
    "\n",
    "信息增益率越大，说明该特征对数据集的划分效果越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb67e8",
   "metadata": {},
   "source": [
    "## 信息增益率相较于信息增益的优势\n",
    "信息增益虽然能够衡量特征对数据集划分的效果，但它存在偏向于取值较多的特征的问题。\n",
    "\n",
    "信息增益率通过引入固有值 (Intrinsic Value, IV) 对信息增益进行归一化，克服了这一偏向性。\n",
    "\n",
    "### 优势：\n",
    "1. **减少偏向性**：信息增益率能够避免信息增益偏向于取值较多的特征。\n",
    "2. **更公平的特征选择**：通过归一化，信息增益率能够更公平地衡量不同特征的划分效果。\n",
    "\n",
    "***因此，在特征取值较多的情况下，信息增益率通常比信息增益更适合作为特征选择的指标。***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ab8ff",
   "metadata": {},
   "source": [
    "## 篮球比赛熵值计算\n",
    "假设有4个球队 {A, B, C, D}，它们的获胜概率分别为 {1/2, 1/4, 1/8, 1/8}，计算其熵值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ade2ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "篮球比赛的熵值 Ent(D): 1.7500\n"
     ]
    }
   ],
   "source": [
    "# 计算熵值\n",
    "probabilities = [1/2, 1/4, 1/8, 1/8]\n",
    "entropy_value = -sum(p * log2(p) for p in probabilities)\n",
    "print(f'篮球比赛的熵值 Ent(D): {entropy_value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eedcdb7",
   "metadata": {},
   "source": [
    "## 信息增益\n",
    "信息增益是衡量某一特征对数据集划分效果的一种指标。信息增益的公式为：\n",
    "$$ Gain = Ent(D) - \\sum_{i=1}^{k} \\frac{|D_i|}{|D|} Ent(D_i) $$\n",
    "其中：\n",
    "- $Entropy(D)$ 是数据集 $D$ 的信息熵。\n",
    "- $D_i$ 是按照某一特征划分后的第 $i$ 个子集。\n",
    "- $|D_i|$ 是子集 $D_i$ 的大小，$|D|$ 是数据集 $D$ 的大小。\n",
    "\n",
    "***信息增益越大，说明该特征对数据集的划分效果越好。***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "acc72f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信息增益: 0.1281\n"
     ]
    }
   ],
   "source": [
    "# 计算信息增益\n",
    "def information_gain(parent_entropy, subsets):\n",
    "    total = sum(len(subset) for subset in subsets)\n",
    "    weighted_entropy = sum((len(subset) / total) * entropy(subset) for subset in subsets)\n",
    "    gain = parent_entropy - weighted_entropy\n",
    "    return gain\n",
    "\n",
    "# 示例数据\n",
    "parent_labels = [0, 0, 1, 1, 1, 0, 1]\n",
    "subset1 = [0, 0, 1]\n",
    "subset2 = [1, 1, 0, 1]\n",
    "\n",
    "parent_entropy = entropy(parent_labels)\n",
    "gain = information_gain(parent_entropy, [subset1, subset2])\n",
    "print(f'信息增益: {gain:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5b124",
   "metadata": {},
   "source": [
    "## CART 剪枝\n",
    "CART (Classification and Regression Tree) 剪枝是决策树算法中用于防止过拟合的重要步骤。\n",
    "\n",
    "### 剪枝的两种方法：\n",
    "1. **预剪枝 (Pre-pruning)**：\n",
    "   - 在构建决策树的过程中提前停止树的生长。\n",
    "   - 通过设置条件（如最大深度、最小样本数等）来限制树的复杂度。\n",
    "\n",
    "2. **后剪枝 (Post-pruning)**：\n",
    "   - 先生成一棵完整的决策树，然后通过剪枝来简化树。\n",
    "   - 剪枝的依据通常是通过交叉验证来评估子树的性能。\n",
    "\n",
    "### 剪枝的优点：\n",
    "- 减少模型复杂度，提升泛化能力。\n",
    "- 防止过拟合，提高模型在测试集上的表现。\n",
    "\n",
    "***CART 剪枝在实际应用中非常重要，能够有效平衡模型的复杂度和性能。***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb31e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction  import DictVectorizer#字典特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be38ebb2",
   "metadata": {},
   "source": [
    "## dict_demo 函数功能解释\n",
    "该函数演示了如何使用 `sklearn.feature_extraction` 中的 `DictVectorizer` 类将字典列表转换为数值特征矩阵。\n",
    "\n",
    "### 功能步骤：\n",
    "1. **输入数据**：函数以一个字典列表作为输入，每个字典包含两个键：`city`（表示城市名称，类别特征）和 `temperature`（表示温度，数值特征）。\n",
    "\n",
    "2. **初始化 DictVectorizer**：通过 `DictVectorizer(sparse=False)` 初始化特征提取器，设置 `sparse=False` 以返回密集矩阵。\n",
    "\n",
    "3. **特征转换**：调用 `fit_transform` 方法将输入数据转换为数值特征矩阵：\n",
    "   - 类别特征（`city`）通过独热编码（One-Hot Encoding）转换为多个二进制列。\n",
    "   - 数值特征（`temperature`）保持不变。\n",
    "\n",
    "4. **输出结果**：\n",
    "   - 打印转换后的数值特征矩阵。\n",
    "   - 打印特征名称列表（通过 `get_feature_names_out` 方法获取）。\n",
    "\n",
    "### 应用场景：\n",
    "该函数展示了如何将混合类型的特征（类别和数值）转换为统一的数值格式，以便用于机器学习模型的训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350a74d",
   "metadata": {},
   "source": [
    "## 密集矩阵解释\n",
    "在 `dict_demo` 函数中，`DictVectorizer` 的 `sparse=False` 参数指定返回密集矩阵。\n",
    "\n",
    "### 什么是密集矩阵？\n",
    "密集矩阵是一种矩阵表示形式，其中所有的元素（包括零值）都显式存储。这种表示方式适用于非稀疏数据，即矩阵中大部分元素为非零值的情况。\n",
    "\n",
    "### 示例：\n",
    "在 `dict_demo` 函数中，输入数据为：\n",
    "```python\n",
    "[\n",
    "    {'city': '北京', 'temperature': 10},\n",
    "    {'city': '上海', 'temperature': 20},\n",
    "    {'city': '深圳', 'temperature': 30}\n",
    "]\n",
    "```\n",
    "通过 `DictVectorizer` 转换后，生成的密集矩阵如下：\n",
    "```\n",
    "[\n",
    " [ 1.  0.  0. 10.],\n",
    " [ 0.  1.  0. 20.],\n",
    " [ 0.  0.  1. 30.]\n",
    "]\n",
    "```\n",
    "其中：\n",
    "- 前三列为 `city` 特征的独热编码（One-Hot Encoding）。\n",
    "- 最后一列为 `temperature` 特征的数值。\n",
    "\n",
    "### 密集矩阵的优点：\n",
    "1. **简单直观**：所有数据显式存储，便于理解和操作。\n",
    "2. **适用于非稀疏数据**：当矩阵中非零值较多时，密集矩阵的存储和计算效率较高。\n",
    "\n",
    "### 注意事项：\n",
    "对于稀疏数据（大部分元素为零），密集矩阵可能会浪费存储空间，此时应使用稀疏矩阵表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8fc8fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_demo():\n",
    "    data = [\n",
    "        {'city': '北京', 'temperature': 10},\n",
    "        {'city': '上海', 'temperature': 20},\n",
    "        {'city': '深圳', 'temperature': 30}\n",
    "    ]\n",
    "    transfer = DictVectorizer(sparse=False)\n",
    "    data = transfer.fit_transform(data)\n",
    "    print(\"返回的结果\\n\",data)\n",
    "    print(\"特征的名字\\n\",transfer.get_feature_names_out(data))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1147e3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "返回的结果\n",
      " [[ 0.  1.  0. 10.]\n",
      " [ 1.  0.  0. 20.]\n",
      " [ 0.  0.  1. 30.]]\n",
      "特征的名字\n",
      " ['city=上海' 'city=北京' 'city=深圳' 'temperature']\n"
     ]
    }
   ],
   "source": [
    "dict_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cfcbf923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0570214a",
   "metadata": {},
   "source": [
    "## CountVectorizer 功能解释\n",
    "`CountVectorizer` 是 `sklearn.feature_extraction.text` 模块中的一个工具，用于将文本数据转换为词频矩阵。\n",
    "\n",
    "### 功能步骤：\n",
    "1. **输入数据**：接受一组文本数据（如字符串列表），每个字符串表示一个文档。\n",
    "\n",
    "2. **分词**：将每个文档分解为单独的词（Token）。\n",
    "\n",
    "3. **构建词汇表**：统计所有文档中出现的唯一词汇，并为每个词分配一个索引。\n",
    "\n",
    "4. **生成词频矩阵**：\n",
    "   - 每行表示一个文档。\n",
    "   - 每列表示一个词汇。\n",
    "   - 矩阵中的值表示该词在对应文档中出现的次数。\n",
    "\n",
    "### 示例：\n",
    "输入数据：\n",
    "```python\n",
    "[\"life is fhort,i like python\", \"life is too long,i dislike python\"]\n",
    "```\n",
    "通过 `CountVectorizer` 转换后：\n",
    "```\n",
    "词汇表：['dislike', 'fhort', 'is', 'life', 'like', 'long', 'python', 'too']\n",
    "词频矩阵：\n",
    "[\n",
    " [0, 1, 1, 1, 1, 0, 1, 0],\n",
    " [1, 0, 1, 1, 0, 1, 1, 1]\n",
    "]\n",
    "```\n",
    "### 应用场景：\n",
    "`CountVectorizer` 常用于自然语言处理（NLP）任务中，将文本数据转换为数值特征，以便用于机器学习模型的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "062d3baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "返回的结果\n",
      " [[0 1 1 1 1 0 1 0]\n",
      " [1 0 1 1 0 1 1 1]]\n",
      "特征的名字\n",
      " ['dislike' 'fhort' 'is' 'life' 'like' 'long' 'python' 'too']\n"
     ]
    }
   ],
   "source": [
    "data=[\"life is fhort,i like python\",\"life is too long,i dislike python\"]\n",
    "transfer=CountVectorizer()\n",
    "data=transfer.fit_transform(data)\n",
    "print(\"返回的结果\\n\",data.toarray())\n",
    "print(\"特征的名字\\n\",transfer.get_feature_names_out(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
