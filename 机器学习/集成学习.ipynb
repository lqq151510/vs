{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "ce24ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8254360c",
   "metadata": {},
   "source": [
    "数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "63050332",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "x, y = iris.data, iris.target\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(x_train)\n",
    "X_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273fe61",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "1cedbc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM准确率: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# 网格搜索\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, cv=5)\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "# 评估模型\n",
    "svm_acc = grid.score(x_test, y_test)\n",
    "print(f\"SVM准确率: {svm_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b40686e",
   "metadata": {},
   "source": [
    "逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "69c8ca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "逻辑回归准确率: 0.9667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuyongze/vs/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "log=LogisticRegression( penalty='l2',  # L2正则化\n",
    "    C=1.0,         # 正则化强度的倒数，C越小正则化越强\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000)\n",
    "log.fit(x_train,y_train)\n",
    "log_acc=log.score(x_test,y_test)#正确预测的样本数占总样本数的比例,分类任务\n",
    "print(f\"逻辑回归准确率: {log_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c16980b",
   "metadata": {},
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "c886c41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn准确率: 0.9667\n"
     ]
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(x_train,y_train)\n",
    "knn_acc=knn.score(x_test,y_test)\n",
    "print(f\"knn准确率: {knn_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d097613",
   "metadata": {},
   "source": [
    "投票集成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "9d98335f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "投票集成准确率: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 创建投票集成模型\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svm_clf' , SVC(kernel='linear', C=1.0, random_state=42)),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "        ('lig_reg2', LogisticRegression(penalty='l2', max_iter=1000))\n",
    "    ],\n",
    "    voting='hard'  # 硬投票\n",
    ")\n",
    "voting_clf.fit(x_train, y_train)\n",
    "voting_acc = voting_clf.score(x_test, y_test)\n",
    "print(f\"投票集成准确率: {voting_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed1732",
   "metadata": {},
   "source": [
    "在这段代码中，我们创建了一个投票分类器 (`VotingClassifier`) 来集成多个基础分类器的预测结果。以下是代码的详细解释：\n",
    "\n",
    "1. **`VotingClassifier`**:\n",
    "   - `VotingClassifier` 是 Scikit-learn 提供的一种集成学习方法，用于结合多个分类器的预测结果以提高整体模型的性能。\n",
    "   - 它支持两种投票方式：硬投票 (`hard voting`) 和软投票 (`soft voting`)。\n",
    "     - 硬投票：根据每个分类器的预测结果进行多数表决，选择票数最多的类别。\n",
    "     - 软投票：根据每个分类器的预测概率进行加权平均，选择概率最高的类别。\n",
    "\n",
    "2. **`estimators` 参数**:\n",
    "   - 这是一个包含多个基础分类器的列表，每个分类器由一个名称和一个模型实例组成。\n",
    "   - 在这里，我们定义了三个基础分类器：\n",
    "     - `'lin_reg'`: 一个逻辑回归分类器 (`LogisticRegression`)，最大迭代次数设置为 1000。\n",
    "     - `'knn'`: 一个 K 最近邻分类器 (`KNeighborsClassifier`)，邻居数设置为 5。\n",
    "     - `'log_reg'`: 另一个逻辑回归分类器，使用 L2 正则化，最大迭代次数设置为 1000。\n",
    "\n",
    "3. **`voting` 参数**:\n",
    "   - 这里设置为 `'hard'`，表示使用硬投票方式。每个基础分类器对样本进行预测后，最终的预测结果由多数表决决定。\n",
    "\n",
    "通过这种方式，投票分类器可以结合多个模型的优势，从而提高分类的准确性和鲁棒性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da86ac8f",
   "metadata": {},
   "source": [
    "随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "cd70b52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机森林准确率: 0.3667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 随机森林模型\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=100,  # 树的数量\n",
    "    max_depth=None,    # 树的最大深度\n",
    "     random_state=42\n",
    ")\n",
    "rf_clf.fit(x_train, y_train)  # 随机森林不需要特征缩放\n",
    "\n",
    "# 评估\n",
    "rf_acc = rf_clf.score(X_test, y_test)\n",
    "print(f\"随机森林准确率: {rf_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbcd5c9",
   "metadata": {},
   "source": [
    "在这段代码中，我们创建了一个随机森林分类器 (`RandomForestClassifier`) 来进行分类任务。以下是代码的详细解释：\n",
    "\n",
    "1. **`RandomForestClassifier`**:\n",
    "   - 随机森林是一种集成学习方法，通过构建多个决策树并结合它们的预测结果来提高模型的准确性和鲁棒性。\n",
    "   - 它通过引入随机性（如随机选择特征和样本）来减少过拟合问题。\n",
    "\n",
    "2. **参数说明**:\n",
    "   - `n_estimators=100`:\n",
    "     - 指定随机森林中包含的决策树数量。在这里，我们设置为 100 棵树。\n",
    "     - 更多的树通常可以提高模型的性能，但也会增加计算成本。\n",
    "   - `max_depth=None`:\n",
    "     - 指定每棵决策树的最大深度。`None` 表示树会一直生长，直到所有叶子节点是纯的，或者叶子节点包含的样本数小于 `min_samples_split`。\n",
    "     - 限制树的深度可以防止过拟合。\n",
    "   - `random_state=42`:\n",
    "     - 设置随机种子，以确保结果的可重复性。\n",
    "\n",
    "3. **随机森林的特点**:\n",
    "   - 随机森林通过对数据集进行多次采样（袋外采样，Bootstrap Sampling）来训练每棵树。\n",
    "   - 每棵树在训练时只使用部分特征，从而增加了模型的多样性。\n",
    "   - 最终的预测结果是通过所有树的投票（分类任务）或平均值（回归任务）得出的。\n",
    "\n",
    "通过这种方式，随机森林能够在保持高准确率的同时，减少单一决策树可能出现的过拟合问题，是一种强大的集成学习方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a95a404",
   "metadata": {},
   "source": [
    "Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "449ce84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost准确率: 0.2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# AdaBoost模型\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    n_estimators=50,    # 弱学习器的最大数量\n",
    "    learning_rate=1.0,  # 学习率\n",
    "    random_state=42\n",
    ")\n",
    "ada_clf.fit(X_train, y_train)  # AdaBoost也不需要特征缩放\n",
    "\n",
    "# 评估\n",
    "ada_acc = ada_clf.score(x_test, y_test)\n",
    "print(f\"AdaBoost准确率: {ada_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e41a89",
   "metadata": {},
   "source": [
    "在这段代码中，我们创建了一个 AdaBoost 分类器 (`AdaBoostClassifier`) 来进行分类任务。以下是代码的详细解释：\n",
    "\n",
    "1. **`AdaBoostClassifier`**:\n",
    "   - AdaBoost（Adaptive Boosting）是一种提升方法，通过组合多个弱学习器（通常是决策树）来构建一个强大的分类器。\n",
    "   - 它通过迭代地调整样本权重，使模型更加关注之前分类错误的样本，从而提高整体性能。\n",
    "\n",
    "2. **参数说明**:\n",
    "   - `n_estimators=50`:\n",
    "     - 指定弱学习器的最大数量。在这里，我们设置为 50 个弱学习器。\n",
    "     - 更多的弱学习器可能会提高模型的性能，但也可能导致过拟合。\n",
    "   - `learning_rate=1.0`:\n",
    "     - 学习率控制每个弱学习器对最终模型的贡献。较小的学习率可以提高模型的鲁棒性，但需要更多的弱学习器。\n",
    "   - `random_state=42`:\n",
    "     - 设置随机种子，以确保结果的可重复性。\n",
    "\n",
    "3. **AdaBoost 的特点**:\n",
    "   - 每次迭代中，AdaBoost 会根据前一轮的分类结果调整样本的权重，增加分类错误样本的权重。\n",
    "   - 最终的分类结果是通过加权投票（分类任务）或加权平均（回归任务）得出的。\n",
    "\n",
    "通过这种方式，AdaBoost 能够有效地提升弱学习器的性能，适用于处理噪声较少的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "8fdf0726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model  Accuracy\n",
      "0     支持向量机  1.000000\n",
      "3      投票集成  1.000000\n",
      "1       KNN  0.966667\n",
      "2      逻辑回归  0.966667\n",
      "4      随机森林  0.366667\n",
      "5  AdaBoost  0.200000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['支持向量机', 'KNN', '逻辑回归', '投票集成', '随机森林', 'AdaBoost'],\n",
    "    'Accuracy': [svm_acc, knn_acc, log_acc, voting_acc, rf_acc, ada_acc]\n",
    "})\n",
    "\n",
    "print(results.sort_values(by='Accuracy', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
