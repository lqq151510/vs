{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b510404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris#数据\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV#划分数据集\n",
    "from sklearn.preprocessing import StandardScaler#数据标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e8c7d",
   "metadata": {},
   "source": [
    "# 线性回归算法\n",
    "线性回归是一种用于建模目标变量与一个或多个特征变量之间关系的统计方法。\n",
    "\n",
    "其公式为：\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n + \\epsilon $$\n",
    "\n",
    "其中：\n",
    "- $y$ 是目标变量\n",
    "- $x_1, x_2, \\dots, x_n$ 是特征变量\n",
    "- $\\beta_0$ 是截距\n",
    "- $\\beta_1, \\beta_2, \\dots, \\beta_n$ 是回归系数\n",
    "- $\\epsilon$ 是误差项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1976403b",
   "metadata": {},
   "source": [
    "# 线性关系和非线性关系\n",
    "在数据分析中，变量之间的关系可以分为线性关系和非线性关系。\n",
    "\n",
    "## 线性关系\n",
    "线性关系是指两个变量之间的关系可以用一条直线来表示，其数学表达式为：\n",
    "\n",
    "$$ y = mx + c $$\n",
    "\n",
    "其中：\n",
    "- $m$ 是斜率，表示变量变化的速率\n",
    "- $c$ 是截距，表示直线与 $y$ 轴的交点\n",
    "\n",
    "## 非线性关系\n",
    "非线性关系是指两个变量之间的关系不能用一条直线来表示，而是需要用曲线来描述。例如：\n",
    "\n",
    "- 二次关系：$$ y = ax^2 + bx + c $$\n",
    "- 指数关系：$$ y = a e^{bx} $$\n",
    "- 对数关系：$$ y = a \\ln(x) + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f261aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d992ef1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([86.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[[80,86],[82,80],[85,78],[90,90],[86,82],[82,90],[78,80]]\n",
    "y=[84.2,80.6,80.1,90,83.2,87.6,79.4]\n",
    "estimator=LinearRegression()\n",
    "estimator.fit(x,y)\n",
    "estimator.coef_\n",
    "estimator.predict([[100,80]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ac6de",
   "metadata": {},
   "source": [
    "这段代码展示了如何使用 `sklearn` 库中的 `LinearRegression` 类来进行线性回归建模和预测。以下是代码的逐步解释：\n",
    "\n",
    "1. **定义输入数据和目标变量**：\n",
    "   ```python\n",
    "   x=[[80,86],[82,80],[85,78],[90,90],[86,82],[82,90],[78,80]]\n",
    "   y=[84.2,80.6,80.1,90,83.2,87.6,79.4]\n",
    "   ```\n",
    "   - `x` 是一个二维列表，表示特征变量的值，每个子列表包含两个特征。\n",
    "   - `y` 是目标变量的值，与 `x` 中的每一行对应。\n",
    "\n",
    "2. **创建线性回归模型**：\n",
    "   ```python\n",
    "   estimator=LinearRegression()\n",
    "   ```\n",
    "   - 使用 `LinearRegression()` 创建一个线性回归模型实例 `estimator`。\n",
    "\n",
    "3. **拟合模型**：\n",
    "   ```python\n",
    "   estimator.fit(x,y)\n",
    "   ```\n",
    "   - 使用 `fit` 方法将模型拟合到数据 `x` 和 `y` 上。模型会学习特征与目标变量之间的关系。\n",
    "\n",
    "4. **获取回归系数**：\n",
    "   ```python\n",
    "   estimator.coef_\n",
    "   ```\n",
    "   - `coef_` 属性返回模型的回归系数（即每个特征的权重）。这些系数表示每个特征对目标变量的影响程度。\n",
    "\n",
    "5. **进行预测**：\n",
    "   ```python\n",
    "   estimator.predict([[100,80]])\n",
    "   ```\n",
    "   - 使用 `predict` 方法对新的输入数据 `[[100,80]]` 进行预测，返回预测的目标变量值。\n",
    "\n",
    "总结来说，这段代码通过线性回归模型对给定的特征和目标变量进行建模，并使用训练好的模型对新数据进行预测。\n",
    "\n",
    "找到具有 1 个许可证类型的类似代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c022302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.7]\n"
     ]
    }
   ],
   "source": [
    "print(estimator.coef_)#打印对应线性回归系数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55802484",
   "metadata": {},
   "source": [
    "# 线性回归的损失和优化\n",
    "在线性回归中，模型的目标是找到一组回归系数，使得预测值与真实值之间的误差最小。\n",
    "\n",
    "## 损失函数\n",
    "线性回归通常使用均方误差（Mean Squared Error, MSE）作为损失函数，其定义为：\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "其中：\n",
    "- $n$ 是样本数量\n",
    "- $y_i$ 是第 $i$ 个样本的真实值\n",
    "- $\\hat{y}_i$ 是第 $i$ 个样本的预测值\n",
    "\n",
    "MSE 衡量了模型预测值与真实值之间的平均平方误差，值越小表示模型的拟合效果越好。\n",
    "\n",
    "## 优化方法\n",
    "为了最小化损失函数，线性回归通常使用以下两种优化方法：\n",
    "\n",
    "1. **正规方程**：\n",
    "   - 对于普通最小二乘法（OLS），可以通过解析解直接计算回归系数：\n",
    "     $$ \\beta = (X^T X)^{-1} X^T y $$\n",
    "   - 这种方法计算速度快，但当特征数量较多或矩阵不可逆时可能不适用。\n",
    "   - 不能解决拟合问题,小规模数据\n",
    "\n",
    "2. **梯度下降**：\n",
    "   - 通过迭代优化的方法逐步更新回归系数：\n",
    "     $$ \\beta := \\beta - \\eta \\nabla L(\\beta) $$\n",
    "   - 其中 $\\eta$ 是学习率，$\\nabla L(\\beta)$ 是损失函数的梯度。\n",
    "   - 梯度下降适用于大规模数据集，但需要选择合适的学习率以确保收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d78ec",
   "metadata": {},
   "source": [
    "# 梯度下降的定义和公式\n",
    "梯度下降是一种优化算法，用于通过迭代的方式最小化目标函数（例如损失函数）。\n",
    "\n",
    "## 定义\n",
    "梯度下降的核心思想是沿着目标函数梯度的反方向更新参数，因为梯度的方向表示函数值增长最快的方向，而反方向则是函数值减小最快的方向。\n",
    "\n",
    "## 公式\n",
    "对于一个目标函数 $L(\\beta)$，梯度下降的更新公式为：\n",
    "\n",
    "$$ \\beta := \\beta - \\eta \\nabla L(\\beta) $$\n",
    "\n",
    "其中：\n",
    "- $\\beta$ 是需要优化的参数（例如线性回归中的回归系数）\n",
    "- $\\eta$ 是学习率，控制每次更新的步长\n",
    "- $\\nabla L(\\beta)$ 是目标函数 $L(\\beta)$ 对参数 $\\beta$ 的梯度,目标函数的微分\n",
    "\n",
    "## 梯度计算\n",
    "以线性回归的均方误差（MSE）为例，其梯度为：\n",
    "\n",
    "$$ \\nabla L(\\beta) = -\\frac{2}{n} X^T (y - X \\beta) $$\n",
    "\n",
    "通过不断更新参数 $\\beta$，梯度下降算法可以逐步逼近目标函数的最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80ea0c",
   "metadata": {},
   "source": [
    "# 正规方程推导\n",
    "在线性回归中，目标是最小化损失函数（通常是均方误差）：\n",
    "\n",
    "$$ L(\\beta) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - X_i \\beta)^2 $$\n",
    "\n",
    "其中：\n",
    "- $y_i$ 是目标变量的真实值\n",
    "- $\\hat{y}_i = X_i \\beta$ 是预测值\n",
    "- $X_i$ 是特征矩阵的第 $i$ 行\n",
    "- $\\beta$ 是回归系数向量\n",
    "\n",
    "将损失函数展开并向量化表示：\n",
    "\n",
    "$$ L(\\beta) = (y - X \\beta)^T (y - X \\beta) $$\n",
    "\n",
    "对 $\\beta$ 求导并令导数为 0：\n",
    "\n",
    "$$ \\frac{\\partial L(\\beta)}{\\partial \\beta} = -2 X^T (y - X \\beta) = 0 $$\n",
    "\n",
    "解得：\n",
    "\n",
    "$$ \\beta = (X^T X)^{-1} X^T y $$\n",
    "\n",
    "这就是线性回归的正规方程。通过正规方程可以直接计算回归系数，无需迭代优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732d03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=LinearRegression(fit_intercept=False)#是否偏置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9834d8",
   "metadata": {},
   "source": [
    "# 线性回归示例\n",
    "本示例展示如何使用 `LinearRegression` 进行建模，并通过均方误差（MSE）和 $R^2$ 分数评估模型性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f90556e",
   "metadata": {},
   "source": [
    "正规方程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20429754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个模型偏置是:\n",
      " -5.592988384093377\n",
      "这个模型的系数是:\n",
      " [40.01091848]\n",
      "均方误差 (MSE): 191.65201592081098\n",
      "R^2 分数: 0.884891993144589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 生成回归数据集\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#正则化\n",
    "transfer=StandardScaler()\n",
    "X_test=transfer.fit_transform(X_test)\n",
    "X_train=transfer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# 创建线性回归模型并训练\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print('这个模型偏置是:\\n',model.intercept_)\n",
    "print('这个模型的系数是:\\n',model.coef_)\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 模型评估\n",
    "mse = mean_squared_error(y_test, y_pred)#均方误差\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"均方误差 (MSE): {mse}\")\n",
    "print(f\"R^2 分数: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf1ff5",
   "metadata": {},
   "source": [
    "梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b6e4626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个模型偏置是:\n",
      " [-2.71956296]\n",
      "这个模型的系数是:\n",
      " [38.47069671]\n",
      "均方误差 (MSE): 140.2017036512064\n",
      "R^2 分数: 0.9157935355519997\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 生成回归数据集\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#正则化\n",
    "transfer=StandardScaler()\n",
    "X_test=transfer.fit_transform(X_test)\n",
    "X_train=transfer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# 创建线性回归模型并训练\n",
    "model = SGDRegressor(max_iter=1000,learning_rate='constant',eta0=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "print('这个模型偏置是:\\n',model.intercept_)\n",
    "print('这个模型的系数是:\\n',model.coef_)\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 模型评估\n",
    "mse = mean_squared_error(y_test, y_pred)#均方误差\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"均方误差 (MSE): {mse}\")\n",
    "print(f\"R^2 分数: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e240e9",
   "metadata": {},
   "source": [
    "`SGDRegressor` 是 `scikit-learn` 中的一个线性模型，用于执行带有随机梯度下降优化的线性回归。它通过迭代的方式更新模型参数，以最小化损失函数（如均方误差）。\n",
    "\n",
    "在这段代码中：\n",
    "- `max_iter=1000` 指定了模型最多迭代 1000 次。这确保了模型有足够的机会收敛到最优解。如果在迭代过程中损失函数的变化小于某个阈值，训练可能会提前停止。\n",
    "- `learning_rate='constant'` 表示学习率保持固定值，不会随着迭代次数动态调整。\n",
    "- `eta0=0.1` 定义了初始学习率的值为 0.1。学习率控制每次参数更新的步长，较大的学习率可能导致收敛不稳定，而较小的学习率可能导致收敛速度较慢。\n",
    "\n",
    "通过这种配置，模型在每次迭代中使用随机样本计算梯度并更新参数，从而在效率和性能之间取得平衡，特别适用于大规模数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084488c1",
   "metadata": {},
   "source": [
    "# 欠拟合、过拟合和正则化\n",
    "\n",
    "## 欠拟合\n",
    "欠拟合是指模型过于简单，无法捕捉数据中的复杂模式，导致训练误差和测试误差都较高。\n",
    "\n",
    "### 特征：\n",
    "- 模型表现不佳，无法很好地拟合训练数据。\n",
    "- 通常是因为模型的复杂度不足或特征不足。\n",
    "\n",
    "### 解决方法：\n",
    "- 增加模型复杂度（例如，使用更多的特征或更复杂的模型）。\n",
    "- 增加训练数据。\n",
    "- 减少正则化强度。\n",
    "\n",
    "## 过拟合\n",
    "过拟合是指模型过于复杂，能够很好地拟合训练数据，但在测试数据上表现较差。\n",
    "\n",
    "### 特征：\n",
    "- 训练误差很低，但测试误差很高。\n",
    "- 模型对训练数据中的噪声或异常值过于敏感。\n",
    "\n",
    "### 解决方法：\n",
    "- 减少模型复杂度（例如，使用更简单的模型或减少特征）。\n",
    "- 增加正则化强度。\n",
    "- 增加训练数据。\n",
    "\n",
    "## 正则化\n",
    "正则化是一种防止过拟合的技术，通过在损失函数中添加惩罚项来限制模型的复杂度。\n",
    "\n",
    "### 常见方法：\n",
    "- **L1 正则化（Lasso 回归）**：通过惩罚系数的绝对值，使部分系数变为零，从而实现特征选择。\n",
    "- **L2 正则化（Ridge 回归）**：通过惩罚系数的平方值，使系数趋向于较小的值。\n",
    "- **Elastic Net**：结合 L1 和 L2 正则化的优点。\n",
    "\n",
    "### 正则化公式：\n",
    "- L1 正则化：$$ L = \\text{MSE} + \\lambda \\sum |w_i| $$\n",
    "- L2 正则化：$$ L = \\text{MSE} + \\lambda \\sum w_i^2 $$\n",
    "\n",
    "其中：\n",
    "- $L$ 是正则化后的损失函数。\n",
    "- $\\lambda$ 是正则化强度的超参数，控制惩罚项的权重。\n",
    "- $w_i$ 是模型的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba7b85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ea5af79",
   "metadata": {},
   "source": [
    "Ridge Regression岭回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22fb9d",
   "metadata": {},
   "source": [
    "# Ridge 回归（岭回归）\n",
    "\n",
    "Ridge 回归是一种线性回归的变体，通过在损失函数中添加 L2 正则化项来限制模型的复杂度，从而防止过拟合。\n",
    "\n",
    "## 损失函数\n",
    "Ridge 回归的损失函数定义为：\n",
    "\n",
    "$$ L = \\text{MSE} + \\lambda \\sum w_i^2 $$\n",
    "\n",
    "其中：\n",
    "- $L$ 是损失函数。\n",
    "- $\\text{MSE}$ 是均方误差。\n",
    "- $\\lambda$ 是正则化强度的超参数，控制正则化项的权重。\n",
    "- $w_i$ 是模型的参数。\n",
    "\n",
    "## 特点\n",
    "- Ridge 回归通过惩罚系数的平方值，使模型的参数趋向于较小的值，从而减少模型的复杂度。\n",
    "- 它适用于特征之间存在多重共线性的问题。\n",
    "- $\\lambda$ 的值越大，正则化强度越高，模型的复杂度越低。\n",
    "\n",
    "## 优点\n",
    "- 防止过拟合，提高模型的泛化能力。\n",
    "- 在高维数据中表现良好。\n",
    "\n",
    "## 缺点\n",
    "- 无法进行特征选择，因为 Ridge 回归不会将系数缩减为零。\n",
    "\n",
    "## 与 Lasso 回归的对比\n",
    "- Ridge 回归使用 L2 正则化，而 Lasso 回归使用 L1 正则化。\n",
    "- Ridge 回归会缩小系数的值，但不会将其缩减为零；Lasso 回归可以将一些系数缩减为零，从而实现特征选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ee4a7d",
   "metadata": {},
   "source": [
    "# Lasso 回归\n",
    "\n",
    "Lasso 回归（Least Absolute Shrinkage and Selection Operator）是一种线性回归的变体，通过在损失函数中添加 L1 正则化项来限制模型的复杂度，同时实现特征选择。\n",
    "\n",
    "## 损失函数\n",
    "Lasso 回归的损失函数定义为：\n",
    "\n",
    "$$ L = \\text{MSE} + \\lambda \\sum |w_i| $$\n",
    "\n",
    "其中：\n",
    "- $L$ 是损失函数。\n",
    "- $\\text{MSE}$ 是均方误差。\n",
    "- $\\lambda$ 是正则化强度的超参数，控制正则化项的权重。\n",
    "- $w_i$ 是模型的参数。\n",
    "\n",
    "## 特点\n",
    "- Lasso 回归通过惩罚系数的绝对值，可以将一些系数缩减为零，从而实现特征选择。\n",
    "- 它适用于高维数据集，尤其是当特征数量多于样本数量时。\n",
    "- $\\lambda$ 的值越大，正则化强度越高，更多的系数会被缩减为零。\n",
    "\n",
    "## 优点\n",
    "- 可以自动选择特征，去除不重要的特征。\n",
    "- 防止过拟合，提高模型的泛化能力。\n",
    "\n",
    "## 缺点\n",
    "- 当特征之间存在高度相关性时，Lasso 回归可能会随机选择其中一个特征，而忽略其他相关特征。\n",
    "\n",
    "## 与 Ridge 回归的对比\n",
    "- Lasso 回归使用 L1 正则化，而 Ridge 回归使用 L2 正则化。\n",
    "- Lasso 回归可以将一些系数缩减为零，从而实现特征选择；而 Ridge 回归只能缩小系数的值，但不会将其缩减为零。\n",
    "\n",
    "## 适用场景\n",
    "- 当需要进行特征选择时，Lasso 回归是一个很好的选择。\n",
    "- 适用于高维数据集，尤其是特征数量远大于样本数量的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d97234",
   "metadata": {},
   "source": [
    "Elastic Net(弹性网络)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc86a32",
   "metadata": {},
   "source": [
    "# Elastic Net（弹性网络）\n",
    "\n",
    "Elastic Net 是一种结合了 L1 正则化（Lasso 回归）和 L2 正则化（Ridge 回归）的线性回归方法，适用于特征数量多且特征之间存在高度相关性的情况。\n",
    "\n",
    "## 损失函数\n",
    "Elastic Net 的损失函数定义为：\n",
    "\n",
    "$$ L = \\text{MSE} + \\alpha \\left( \\rho \\sum |w_i| + \\frac{1 - \\rho}{2} \\sum w_i^2 \\right) $$\n",
    "\n",
    "其中：\n",
    "- $L$ 是损失函数。\n",
    "- $\\text{MSE}$ 是均方误差。\n",
    "- $\\alpha$ 是正则化强度的超参数，控制正则化项的权重。\n",
    "- $\\rho$ 是 L1 和 L2 正则化的混合比例，取值范围为 $[0, 1]$。\n",
    "- $w_i$ 是模型的参数。\n",
    "\n",
    "## 特点\n",
    "- Elastic Net 同时具有 Lasso 和 Ridge 的优点：\n",
    "  - L1 正则化可以实现特征选择。\n",
    "  - L2 正则化可以处理特征之间的多重共线性。\n",
    "- 通过调整 $\\rho$，可以控制 L1 和 L2 正则化的相对重要性。\n",
    "\n",
    "## 优点\n",
    "- 适用于高维数据集，尤其是特征数量多于样本数量的情况。\n",
    "- 在特征之间存在高度相关性时表现良好。\n",
    "- 可以同时进行特征选择和防止过拟合。\n",
    "\n",
    "## 缺点\n",
    "- 需要调节两个超参数（$\\alpha$ 和 $\\rho$），增加了模型的复杂性。\n",
    "\n",
    "## 适用场景\n",
    "- 当特征数量多且特征之间存在相关性时，Elastic Net 是一个很好的选择。\n",
    "- 适用于需要同时进行特征选择和防止过拟合的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33e82f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个模型偏置是:\n",
      " 0.0\n",
      "这个模型的系数是:\n",
      " [39.51695652]\n",
      "均方误差 (MSE): 103.13749308660324\n",
      "R^2 分数: 0.9380546497034081\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 生成回归数据集\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#正则化\n",
    "transfer=StandardScaler()\n",
    "X_test=transfer.fit_transform(X_test)\n",
    "X_train=transfer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# 创建线性回归模型并训练\n",
    "model = Ridge(alpha=1,fit_intercept=False,solver='auto')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print('这个模型偏置是:\\n',model.intercept_)\n",
    "print('这个模型的系数是:\\n',model.coef_)\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 模型评估\n",
    "mse = mean_squared_error(y_test, y_pred)#均方误差\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"均方误差 (MSE): {mse}\")\n",
    "print(f\"R^2 分数: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7fdbdb",
   "metadata": {},
   "source": [
    "`Ridge` 是 `scikit-learn` 中的一个线性回归模型，用于执行带有 L2 正则化的回归分析。L2 正则化通过在损失函数中添加系数平方和的惩罚项，限制模型的复杂度，从而防止过拟合。\n",
    "\n",
    "在这段代码中：\n",
    "- `alpha=1` 指定了正则化强度。`alpha` 值越大，正则化效果越强，模型的系数会被压缩得越小，从而降低模型的复杂度。\n",
    "- `fit_intercept=True` 表示模型会计算截距项。如果设置为 `False`，模型将不会计算截距，假设数据已经居中。\n",
    "- `solver='auto'` 表示自动选择求解器。`Ridge` 会根据数据的特性选择最合适的优化算法，例如 `svd` 或 `cholesky`。\n",
    "\n",
    "通过这种配置，`Ridge` 模型可以在防止过拟合的同时保留所有特征，适用于特征之间存在多重共线性的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2bf295",
   "metadata": {},
   "source": [
    "# RidgeCV 和 Ridge 的区别\n",
    "\n",
    "`RidgeCV` 和 `Ridge` 都是 `scikit-learn` 中用于线性回归的模型，二者的主要区别在于是否自动选择正则化强度（`alpha`）。\n",
    "\n",
    "## Ridge\n",
    "- `Ridge` 是带有 L2 正则化的线性回归模型。\n",
    "- 用户需要手动指定正则化强度 `alpha`。\n",
    "- 适用于用户已经确定好最佳 `alpha` 值的情况。\n",
    "\n",
    "## RidgeCV\n",
    "- `RidgeCV` 是带有交叉验证功能的 Ridge 回归模型。\n",
    "- 它会在给定的一组 `alpha` 候选值中，通过交叉验证自动选择最佳的 `alpha` 值。\n",
    "- 适用于用户不确定最佳 `alpha` 值的情况。\n",
    "\n",
    "## 使用场景\n",
    "- 如果用户已经通过其他方法确定了最佳 `alpha` 值，可以直接使用 `Ridge`。\n",
    "- 如果用户希望模型自动选择最佳的正则化强度，可以使用 `RidgeCV`。\n",
    "\n",
    "## 示例\n",
    "```python\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "\n",
    "# 使用 Ridge\n",
    "ridge_model = Ridge(alpha=1)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# 使用 RidgeCV\n",
    "ridgecv_model = RidgeCV(alphas=[0.1, 1, 10])\n",
    "ridgecv_model.fit(X_train, y_train)\n",
    "print('最佳 alpha:', ridgecv_model.alpha_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b206d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个模型偏置是:\n",
      " -5.592988384093377\n",
      "这个模型的系数是:\n",
      " [40.00591774]\n",
      "均方误差 (MSE): 191.6504549102545\n",
      "R^2 分数: 0.88489293070224\n",
      "最佳 alpha: 0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 生成回归数据集\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#正则化\n",
    "transfer=StandardScaler()\n",
    "X_test=transfer.fit_transform(X_test)\n",
    "X_train=transfer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# 创建线性回归模型并训练\n",
    "model = RidgeCV(alphas=(0.001,0.01,0.1,1,10,100))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print('这个模型偏置是:\\n',model.intercept_)\n",
    "print('这个模型的系数是:\\n',model.coef_)\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 模型评估\n",
    "mse = mean_squared_error(y_test, y_pred)#均方误差\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"均方误差 (MSE): {mse}\")\n",
    "print(f\"R^2 分数: {r2}\")\n",
    "print('最佳 alpha:', model.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a2d270",
   "metadata": {},
   "source": [
    "模型的保存和加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5aed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个模型偏置是:\n",
      " 0.0\n",
      "这个模型的系数是:\n",
      " [39.51695652]\n",
      "均方误差 (MSE): 103.13749308660324\n",
      "R^2 分数: 0.9380546497034081\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# 生成回归数据集\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#正则化\n",
    "transfer=StandardScaler()\n",
    "X_test=transfer.fit_transform(X_test)\n",
    "X_train=transfer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# 创建线性回归模型并训练\n",
    "model = Ridge(alpha=1,fit_intercept=False,solver='auto')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "# joblib.dump(model,'test.pkl')#模型保存\n",
    "# model=joblib.load('test.pkl')#模型加载\n",
    "print('这个模型偏置是:\\n',model.intercept_)\n",
    "print('这个模型的系数是:\\n',model.coef_)\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 模型评估\n",
    "mse = mean_squared_error(y_test, y_pred)#均方误差\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"均方误差 (MSE): {mse}\")\n",
    "print(f\"R^2 分数: {r2}\")\n",
    "estimator=joblib.load('test.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
